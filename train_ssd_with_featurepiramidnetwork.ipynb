{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from utils.ssd import make_vgg, make_extras, L2Norm, make_loc_conf, DBox\n",
    "\n",
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "N = range(10)\n",
    "Z = range(10,20)\n",
    "for i,ii in zip(reversed(Z), reversed(N)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPN(sources, fpnconv):\n",
    "    mode = \"nearest\"\n",
    "    # make layers\n",
    "    sources[5] = fpnconv[0](sources[5])\n",
    "    x = nn.functional.interpolate(sources[5], size=[3,3], mode=mode)\n",
    "    \n",
    "    sources[4] = fpnconv[1](sources[4]) + x\n",
    "    x = nn.functional.interpolate(sources[4], size=[5,5], mode=mode)\n",
    "    \n",
    "    sources[3] = fpnconv[2](sources[3]) + x\n",
    "    x = nn.functional.interpolate(sources[3], size=[10,10], mode=mode)\n",
    "    \n",
    "    sources[2] = fpnconv[3](sources[2]) + x\n",
    "    x = nn.functional.interpolate(sources[2], size=[19,19], mode=mode)\n",
    "    \n",
    "    sources[1] = fpnconv[4](sources[1]) + x\n",
    "    x = nn.functional.interpolate(sources[1], size=[38,38], mode=mode)\n",
    "    \n",
    "    sources[0] = fpnconv[5](sources[0]) + x\n",
    "    \n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n",
    "\n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "\n",
    "    # VGGの22層目、conv4_3（source1）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[0]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[0]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # VGGの最終層（source2）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[1]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[1]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # extraの（source3）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[2]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[2]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # extraの（source4）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # extraの（source5）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # extraの（source6）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNSSD(nn.Module):\n",
    "    def __init__(self, phase, cfg):\n",
    "        super(FPNSSD, self).__init__()\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.num_classes = cfg[\"num_classes\"]\n",
    "        \n",
    "        # call SSD network\n",
    "        self.vgg = make_vgg()\n",
    "        self.extras = make_extras()\n",
    "        self.L2Norm = L2Norm()\n",
    "        self.loc, self.conf = make_loc_conf(self.num_classes, cfg[\"bbox_aspect_num\"])\n",
    "        \n",
    "        #self.FPN = FPN()\n",
    "        \n",
    "        mode = \"nearest\"\n",
    "        self.upsamplers = [\n",
    "            [38,38], [19,19], [10,10], [5,5], [3,3]\n",
    "            ]\n",
    "        \n",
    "        self.fpnconv = nn.ModuleList([\n",
    "            nn.Conv2d(256, 256, kernel_size=1),\n",
    "            nn.Conv2d(256, 256, kernel_size=1),\n",
    "            nn.Conv2d(256, 256, kernel_size=1),\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.Conv2d(1024, 256, kernel_size=1),\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "        ])\n",
    "        \n",
    "        # make Dbox\n",
    "        dbox = DBox(cfg)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dbox_list = dbox.make_dbox_list()\n",
    "        \n",
    "        # use Detect if inference\n",
    "        if phase == \"inference\":\n",
    "            self.detect = Detect()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        sources = list()\n",
    "        loc = list()\n",
    "        conf = list()\n",
    "        \n",
    "        # VGGのconv4_3まで計算\n",
    "        for k in range(23):\n",
    "            x = self.vgg[k](x)\n",
    "        \n",
    "        # conv4_3の出力をL2Normに入力。source1をsourceに追加\n",
    "        source1 = self.L2Norm(x)\n",
    "        sources.append(source1)\n",
    "        \n",
    "        # VGGを最後まで計算しsource2を取得\n",
    "        for k in range(23, len(self.vgg)):\n",
    "            x = self.vgg[k](x)\n",
    "        \n",
    "        sources.append(x)\n",
    "        \n",
    "        # extra層の計算を行う。\n",
    "        # source3-6に結果を格納。\n",
    "        for k, v in enumerate(self.extras):\n",
    "            x = F.relu(v(x), inplace = True)\n",
    "            if k % 2 == 1:\n",
    "                sources.append(x)\n",
    "        \n",
    "        # source1 38x38\n",
    "        # source2 19x19\n",
    "        # source3 10x10\n",
    "        # source4 5x5\n",
    "        # source5 3x3\n",
    "        # source6 1x1\n",
    "        \n",
    "        ## feature piramidレイヤを作成する。\n",
    "        sources = FPN(sources, self.fpnconv)\n",
    "        \n",
    "        # source 1-6にそれぞれ対応するconvを適応しconfとlocを得る。\n",
    "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
    "            # Permuteは要素の順番を入れ替え\n",
    "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
    "        \n",
    "        # convの出力は[batch, 4*anker, fh, fw]なので整形しなければならない。\n",
    "        # まず[batch, fh, fw, anker]に整形\n",
    "        \n",
    "        # locとconfの形を変形\n",
    "        # locのサイズは、torch.Size([batch_num, 34928])\n",
    "        # confのサイズはtorch.Size([batch_num, 183372])になる\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "        \n",
    "        # さらにlocとconfの形を整える\n",
    "        # locのサイズは、torch.Size([batch_num, 8732, 4])\n",
    "        # confのサイズは、torch.Size([batch_num, 8732, 21])\n",
    "        loc = loc.view(loc.size(0), -1, 4)\n",
    "        conf = conf.view(conf.size(0), -1, self.num_classes)\n",
    "        # これで後段の処理につっこめるかたちになる。\n",
    "        \n",
    "        output = (loc, conf, self.dbox_list)\n",
    "        \n",
    "        if self.phase == \"inference\":\n",
    "            # Detectのforward\n",
    "            return self.detect(output[0], output[1], output[2].to(self.device))\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 300,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
    "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = FPNSSD(phase=\"train\", cfg=ssd_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPNSSD(\n",
      "  (vgg): ModuleList(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "    (32): ReLU(inplace)\n",
      "    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (34): ReLU(inplace)\n",
      "  )\n",
      "  (extras): ModuleList(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (L2Norm): L2Norm()\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (fpnconv): ModuleList(\n",
      "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using vgg weights\n",
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "# SSDのweightsを設定\n",
    "print(\"using vgg weights\")\n",
    "vgg_weights = torch.load(\"./weights/vgg16_reducedfc.pth\")\n",
    "net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "# 初期値を適応\n",
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.focalloss import FocalLoss\n",
    "from utils.ssd_model import match\n",
    "\n",
    "# define loss\n",
    "criterion = FocalLoss(num_classes=21)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = 1e-3\n",
    "    for i,lr_decay_epoch in enumerate([120,180]):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainlist:  16551\n",
      "vallist:  0\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn\n",
    "# load files\n",
    "# set your VOCdevkit path!\n",
    "vocpath = \"../VOCdevkit/VOC2007\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath)\n",
    "\n",
    "vocpath = \"../VOCdevkit/VOC2012\"\n",
    "train_img_list2, train_anno_list2, _, _ = make_datapath_list(vocpath)\n",
    "\n",
    "train_img_list.extend(train_img_list2)\n",
    "train_anno_list.extend(train_anno_list2)\n",
    "\n",
    "print(\"trainlist: \", len(train_img_list))\n",
    "print(\"vallist: \", len(val_img_list))\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 300  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "# Dataloaderに入れるデータセットファイル。\n",
    "# ゲットで叩くと画像とGTを前処理して出力してくれる。\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('（train）')\n",
    "            else:\n",
    "                if((epoch+1) % 10 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('（val）')\n",
    "                else:\n",
    "                    # 検証は10回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loc_data, conf_data, dbox_list = outputs\n",
    "                    # 要素数を把握\n",
    "                    num_batch = loc_data.size(0)  # ミニバッチのサイズ\n",
    "                    num_dbox = loc_data.size(1)  # DBoxの数 = 8732\n",
    "                    num_classes = conf_data.size(2)  # クラス数 = 21\n",
    "                    # get target\n",
    "                    conf_t_label = torch.LongTensor(num_batch, num_dbox).to(device)\n",
    "                    loc_t = torch.Tensor(num_batch, num_dbox, 4).to(device)                    \n",
    "                    for idx in range(num_batch):  # ミニバッチでループ\n",
    "                        # 現在のミニバッチの正解アノテーションのBBoxとラベルを取得\n",
    "                        truths = targets[idx][:, :-1].to(device)  # BBox\n",
    "                        # ラベル [物体1のラベル, 物体2のラベル, …]\n",
    "                        labels = targets[idx][:, -1].to(device)\n",
    "\n",
    "                        # デフォルトボックスを新たな変数で用意\n",
    "                        dbox = dbox_list.to(device)\n",
    "\n",
    "                        # 関数matchを実行し、loc_tとconf_t_labelの内容を更新する\n",
    "                        # （詳細）\n",
    "                        # loc_t:各DBoxに一番近い正解のBBoxの位置情報が上書きされる\n",
    "                        # conf_t_label：各DBoxに一番近いBBoxのラベルが上書きされる\n",
    "                        # ただし、一番近いBBoxとのjaccard overlapが0.5より小さい場合は\n",
    "                        # 正解BBoxのラベルconf_t_labelは背景クラスの0とする\n",
    "                        variance = [0.1, 0.2]\n",
    "                        # このvarianceはDBoxからBBoxに補正計算する際に使用する式の係数です\n",
    "                        match(0.5, truths, dbox,\n",
    "                              variance, labels, loc_t, conf_t_label, idx)\n",
    "                    \n",
    "                    # compute focal loss\n",
    "                    loss_l, loss_c = criterion(loc_data, loc_t, conf_data, conf_t_label)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/ssd_fpn_300_' +\n",
    "                       str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device： cuda:0\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "（train）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション 10 || Loss: 94.0714 || 10iter: 3.1381 sec.\n",
      "イテレーション 20 || Loss: 312.3217 || 10iter: 1.6879 sec.\n",
      "イテレーション 30 || Loss: 977.7494 || 10iter: 1.7770 sec.\n",
      "イテレーション 40 || Loss: 188.5627 || 10iter: 1.5916 sec.\n",
      "イテレーション 50 || Loss: 54.6245 || 10iter: 1.5478 sec.\n",
      "イテレーション 60 || Loss: 18.5208 || 10iter: 1.8688 sec.\n",
      "イテレーション 70 || Loss: 26.8983 || 10iter: 1.5191 sec.\n",
      "イテレーション 80 || Loss: 7.3837 || 10iter: 1.7568 sec.\n",
      "イテレーション 90 || Loss: 7.4457 || 10iter: 1.6532 sec.\n",
      "イテレーション 100 || Loss: 5.4962 || 10iter: 1.6441 sec.\n",
      "イテレーション 110 || Loss: 5.1212 || 10iter: 1.7351 sec.\n",
      "イテレーション 120 || Loss: 6.2836 || 10iter: 1.4906 sec.\n",
      "イテレーション 130 || Loss: 4.1422 || 10iter: 1.6876 sec.\n",
      "イテレーション 140 || Loss: 3.3137 || 10iter: 1.7241 sec.\n",
      "イテレーション 150 || Loss: 3.5952 || 10iter: 1.6818 sec.\n",
      "イテレーション 160 || Loss: 3.5138 || 10iter: 1.6875 sec.\n",
      "イテレーション 170 || Loss: 3.6078 || 10iter: 1.7111 sec.\n",
      "イテレーション 180 || Loss: 3.3054 || 10iter: 1.8449 sec.\n",
      "イテレーション 190 || Loss: 3.5862 || 10iter: 1.8054 sec.\n",
      "イテレーション 200 || Loss: 2.9789 || 10iter: 1.6844 sec.\n",
      "イテレーション 210 || Loss: 3.0744 || 10iter: 1.6063 sec.\n",
      "イテレーション 220 || Loss: 3.2559 || 10iter: 1.6088 sec.\n",
      "イテレーション 230 || Loss: 3.3113 || 10iter: 1.5368 sec.\n",
      "イテレーション 240 || Loss: 3.8182 || 10iter: 1.6791 sec.\n",
      "イテレーション 250 || Loss: 3.9721 || 10iter: 1.6745 sec.\n",
      "イテレーション 260 || Loss: 3.1929 || 10iter: 1.6897 sec.\n",
      "イテレーション 270 || Loss: 4.0275 || 10iter: 1.7182 sec.\n",
      "イテレーション 280 || Loss: 2.8964 || 10iter: 1.7974 sec.\n",
      "イテレーション 290 || Loss: 2.5306 || 10iter: 1.5573 sec.\n",
      "イテレーション 300 || Loss: 3.0701 || 10iter: 1.7215 sec.\n",
      "イテレーション 310 || Loss: 2.5191 || 10iter: 1.5872 sec.\n",
      "イテレーション 320 || Loss: 2.8368 || 10iter: 1.6892 sec.\n",
      "イテレーション 330 || Loss: 4.9932 || 10iter: 1.6767 sec.\n",
      "イテレーション 340 || Loss: 3.6660 || 10iter: 1.6073 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
