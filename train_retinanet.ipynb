{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = \"resnet50\"\n",
    "scale = 2 \n",
    "# scale==1: resolution 300\n",
    "# scale==2: resolution 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make data.Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainlist:  16551\n",
      "vallist:  4952\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "# set your VOCdevkit path!\n",
    "vocpath = \"../VOCdevkit/VOC2007\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath)\n",
    "\n",
    "vocpath = \"../VOCdevkit/VOC2012\"\n",
    "train_img_list2, train_anno_list2, _, _ = make_datapath_list(vocpath)\n",
    "\n",
    "train_img_list.extend(train_img_list2)\n",
    "train_anno_list.extend(train_anno_list2)\n",
    "\n",
    "print(\"trainlist: \", len(train_img_list))\n",
    "print(\"vallist: \", len(val_img_list))\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 300*scale  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "# Dataloaderに入れるデータセットファイル。\n",
    "# ゲットで叩くと画像とGTを前処理して出力してくれる。\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 600, 600])\n",
      "16\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# 動作の確認\n",
    "batch_iterator = iter(dataloaders_dict[\"val\"])  # イタレータに変換\n",
    "images, targets = next(batch_iterator)  # 1番目の要素を取り出す\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # ミニバッチのサイズのリスト、各要素は[n, 5]、nは物体数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define SSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.retinanet import RetinaFPN as SSD\n",
    "from utils.retinanet import Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size: torch.Size([1, 256, 75, 75])\n",
      "layer size: torch.Size([1, 256, 38, 38])\n",
      "layer size: torch.Size([1, 256, 19, 19])\n",
      "layer size: torch.Size([1, 256, 10, 10])\n",
      "layer size: torch.Size([1, 256, 5, 5])\n",
      "layer size: torch.Size([1, 256, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1804, -0.1486,  0.1303,  0.2200],\n",
       "          [-0.0645, -0.0586, -0.3193,  0.0754],\n",
       "          [-0.0402, -0.2396, -0.0114,  0.0661],\n",
       "          ...,\n",
       "          [ 0.0448, -0.0307, -0.0462,  0.0433],\n",
       "          [-0.0200,  0.0512,  0.0387,  0.0380],\n",
       "          [-0.0041,  0.0390,  0.0319,  0.0450]]], grad_fn=<ViewBackward>),\n",
       " tensor([[[ 0.1156, -0.3100,  0.1033,  ...,  0.2048,  0.2719,  0.1458],\n",
       "          [-0.1009,  0.0542, -0.2932,  ..., -0.0154, -0.2245,  0.3272],\n",
       "          [-0.1375,  0.0777,  0.0208,  ...,  0.2686, -0.3814,  0.1226],\n",
       "          ...,\n",
       "          [ 0.0064,  0.0448,  0.0303,  ..., -0.0235,  0.0023, -0.0137],\n",
       "          [-0.0238,  0.0041, -0.0016,  ...,  0.0302,  0.0225,  0.0652],\n",
       "          [ 0.0637, -0.0090,  0.0159,  ...,  0.0209,  0.0354, -0.0230]]],\n",
       "        grad_fn=<ViewBackward>),\n",
       " tensor([[0.0067, 0.0067, 0.0500, 0.0500],\n",
       "         [0.0067, 0.0067, 0.0707, 0.0707],\n",
       "         [0.0067, 0.0067, 0.0707, 0.0354],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 0.4806, 0.4806],\n",
       "         [1.0000, 1.0000, 0.6223, 0.3111],\n",
       "         [1.0000, 1.0000, 0.3111, 0.6223]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if scale==1:\n",
    "    ssd_cfg = {\n",
    "        'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
    "        'input_size': 300*scale,  # 画像の入力サイズ\n",
    "        'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "        'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
    "        'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "        'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "        'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "        'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    }\n",
    "elif scale==2:\n",
    "    ssd_cfg = {\n",
    "        'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
    "        'input_size': 300*scale,  # 画像の入力サイズ\n",
    "        'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "        'feature_maps': [75, 38, 19, 10, 5, 3],  # 各sourceの画像サイズ\n",
    "        'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "        'min_sizes': [30, 60, 111, 162, 213, 264]*scale,  # DBOXの大きさを決める\n",
    "        'max_sizes': [60, 111, 162, 213, 264, 315]*scale,  # DBOXの大きさを決める\n",
    "        'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    }\n",
    "\n",
    "# test if net works\n",
    "net = SSD(phase=\"train\", cfg=ssd_cfg, model=backbone, verbose=True)\n",
    "net(torch.rand([1,3,600,600]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "net = SSD(phase=\"train\", cfg=ssd_cfg, model=backbone, verbose=False)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetinaFPN(\n",
      "  (layer0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (toplayer): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (smooth1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (smooth2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (latlayer1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (latlayer2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = 1e-3\n",
    "    for i,lr_decay_epoch in enumerate([120,180]):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device:\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('(train)')\n",
    "            else:\n",
    "                if((epoch+1) % 10 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('(val)')\n",
    "                else:\n",
    "                    # 検証は10回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iter {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/retinanet' + str(300*scale) + \"_\" + \n",
    "                       str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device: cuda:0\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10 || Loss: 15.1184 || 10iter: 8.8231 sec.\n",
      "Iter 20 || Loss: 11.7034 || 10iter: 4.5247 sec.\n",
      "Iter 30 || Loss: 8.7539 || 10iter: 4.5405 sec.\n",
      "Iter 40 || Loss: 9.1248 || 10iter: 4.5388 sec.\n",
      "Iter 50 || Loss: 8.7309 || 10iter: 4.5423 sec.\n",
      "Iter 60 || Loss: 7.9078 || 10iter: 4.5409 sec.\n",
      "Iter 70 || Loss: 7.8319 || 10iter: 5.3253 sec.\n",
      "Iter 80 || Loss: 8.4322 || 10iter: 4.5846 sec.\n",
      "Iter 90 || Loss: 8.3300 || 10iter: 4.6009 sec.\n",
      "Iter 100 || Loss: 7.9276 || 10iter: 4.5954 sec.\n",
      "Iter 110 || Loss: 8.3744 || 10iter: 4.5519 sec.\n",
      "Iter 120 || Loss: 8.6325 || 10iter: 4.5450 sec.\n",
      "Iter 130 || Loss: 7.9593 || 10iter: 4.5543 sec.\n",
      "Iter 140 || Loss: 7.8965 || 10iter: 4.5513 sec.\n",
      "Iter 150 || Loss: 7.1805 || 10iter: 4.5505 sec.\n",
      "Iter 160 || Loss: 8.0453 || 10iter: 4.5604 sec.\n",
      "Iter 170 || Loss: 8.0388 || 10iter: 4.5619 sec.\n",
      "Iter 180 || Loss: 7.5218 || 10iter: 4.7218 sec.\n",
      "Iter 190 || Loss: 6.8632 || 10iter: 4.8312 sec.\n",
      "Iter 200 || Loss: 7.5156 || 10iter: 4.5648 sec.\n",
      "Iter 210 || Loss: 7.0827 || 10iter: 4.5783 sec.\n",
      "Iter 220 || Loss: 6.5841 || 10iter: 4.5977 sec.\n",
      "Iter 230 || Loss: 7.7735 || 10iter: 4.5483 sec.\n",
      "Iter 240 || Loss: 6.8238 || 10iter: 4.5716 sec.\n",
      "Iter 250 || Loss: 7.6296 || 10iter: 4.5591 sec.\n",
      "Iter 260 || Loss: 7.3258 || 10iter: 4.5668 sec.\n",
      "Iter 270 || Loss: 6.4639 || 10iter: 4.5579 sec.\n",
      "Iter 280 || Loss: 7.1183 || 10iter: 4.5673 sec.\n",
      "Iter 290 || Loss: 6.4205 || 10iter: 4.5875 sec.\n",
      "Iter 300 || Loss: 6.9025 || 10iter: 4.5913 sec.\n",
      "Iter 310 || Loss: 7.0916 || 10iter: 4.5468 sec.\n",
      "Iter 320 || Loss: 6.8773 || 10iter: 4.5483 sec.\n",
      "Iter 330 || Loss: 6.3021 || 10iter: 4.5497 sec.\n",
      "Iter 340 || Loss: 6.5941 || 10iter: 4.5631 sec.\n",
      "Iter 350 || Loss: 6.7989 || 10iter: 4.5624 sec.\n",
      "Iter 360 || Loss: 7.4172 || 10iter: 4.5643 sec.\n",
      "Iter 370 || Loss: 7.3879 || 10iter: 4.5655 sec.\n",
      "Iter 380 || Loss: 7.1461 || 10iter: 4.6099 sec.\n",
      "Iter 390 || Loss: 6.6025 || 10iter: 4.5752 sec.\n",
      "Iter 400 || Loss: 6.8261 || 10iter: 4.5735 sec.\n",
      "Iter 410 || Loss: 6.4181 || 10iter: 4.5987 sec.\n",
      "Iter 420 || Loss: 6.5637 || 10iter: 4.5687 sec.\n",
      "Iter 430 || Loss: 6.7426 || 10iter: 4.5482 sec.\n",
      "Iter 440 || Loss: 6.1453 || 10iter: 4.5430 sec.\n",
      "Iter 450 || Loss: 6.7672 || 10iter: 4.5407 sec.\n",
      "Iter 460 || Loss: 7.2946 || 10iter: 4.5467 sec.\n",
      "Iter 470 || Loss: 6.5115 || 10iter: 4.5460 sec.\n",
      "Iter 480 || Loss: 6.4539 || 10iter: 4.5507 sec.\n",
      "Iter 490 || Loss: 6.7480 || 10iter: 4.5545 sec.\n",
      "Iter 500 || Loss: 7.0367 || 10iter: 4.5592 sec.\n",
      "Iter 510 || Loss: 5.9963 || 10iter: 4.5537 sec.\n",
      "Iter 520 || Loss: 6.5181 || 10iter: 4.5789 sec.\n",
      "Iter 530 || Loss: 6.3858 || 10iter: 4.5608 sec.\n",
      "Iter 540 || Loss: 5.6194 || 10iter: 4.5574 sec.\n",
      "Iter 550 || Loss: 6.2747 || 10iter: 4.5467 sec.\n",
      "Iter 560 || Loss: 5.7412 || 10iter: 4.5707 sec.\n",
      "Iter 570 || Loss: 7.0786 || 10iter: 4.5761 sec.\n",
      "Iter 580 || Loss: 6.6826 || 10iter: 4.5790 sec.\n",
      "Iter 590 || Loss: 6.5603 || 10iter: 4.5627 sec.\n",
      "Iter 600 || Loss: 5.6360 || 10iter: 4.5675 sec.\n",
      "Iter 610 || Loss: 6.2748 || 10iter: 4.5860 sec.\n",
      "Iter 620 || Loss: 6.1146 || 10iter: 4.5683 sec.\n",
      "Iter 630 || Loss: 6.3630 || 10iter: 4.5479 sec.\n",
      "Iter 640 || Loss: 6.5767 || 10iter: 4.5864 sec.\n",
      "Iter 650 || Loss: 6.3674 || 10iter: 4.5691 sec.\n",
      "Iter 660 || Loss: 6.4908 || 10iter: 4.5703 sec.\n",
      "Iter 670 || Loss: 5.3449 || 10iter: 4.6045 sec.\n",
      "Iter 680 || Loss: 6.1895 || 10iter: 4.5553 sec.\n",
      "Iter 690 || Loss: 6.8424 || 10iter: 4.5598 sec.\n",
      "Iter 700 || Loss: 6.2825 || 10iter: 4.5501 sec.\n",
      "Iter 710 || Loss: 5.9322 || 10iter: 4.5522 sec.\n",
      "Iter 720 || Loss: 5.8035 || 10iter: 4.5481 sec.\n",
      "Iter 730 || Loss: 5.6913 || 10iter: 4.5511 sec.\n",
      "Iter 740 || Loss: 6.1837 || 10iter: 4.5513 sec.\n",
      "Iter 750 || Loss: 6.0597 || 10iter: 4.5533 sec.\n",
      "Iter 760 || Loss: 5.7300 || 10iter: 4.5543 sec.\n",
      "Iter 770 || Loss: 6.1098 || 10iter: 4.5716 sec.\n",
      "Iter 780 || Loss: 6.4623 || 10iter: 4.5535 sec.\n",
      "Iter 790 || Loss: 5.9838 || 10iter: 4.5576 sec.\n",
      "Iter 800 || Loss: 6.4584 || 10iter: 4.5587 sec.\n",
      "Iter 810 || Loss: 6.8458 || 10iter: 4.5569 sec.\n",
      "Iter 820 || Loss: 5.5835 || 10iter: 4.5541 sec.\n",
      "Iter 830 || Loss: 5.2948 || 10iter: 4.5562 sec.\n",
      "Iter 840 || Loss: 6.0091 || 10iter: 4.6289 sec.\n",
      "Iter 850 || Loss: 5.5997 || 10iter: 4.5667 sec.\n",
      "Iter 860 || Loss: 5.5948 || 10iter: 4.5798 sec.\n",
      "Iter 870 || Loss: 6.7705 || 10iter: 4.5566 sec.\n",
      "Iter 880 || Loss: 5.3689 || 10iter: 4.5568 sec.\n",
      "Iter 890 || Loss: 6.4838 || 10iter: 4.5564 sec.\n",
      "Iter 900 || Loss: 5.2264 || 10iter: 4.5771 sec.\n",
      "Iter 910 || Loss: 5.9558 || 10iter: 4.5598 sec.\n",
      "Iter 920 || Loss: 6.3305 || 10iter: 4.5636 sec.\n",
      "Iter 930 || Loss: 6.0387 || 10iter: 4.5608 sec.\n",
      "Iter 940 || Loss: 5.3390 || 10iter: 4.5528 sec.\n",
      "Iter 950 || Loss: 6.5170 || 10iter: 4.5556 sec.\n",
      "Iter 960 || Loss: 6.5039 || 10iter: 4.5700 sec.\n",
      "Iter 970 || Loss: 5.6031 || 10iter: 4.5636 sec.\n",
      "Iter 980 || Loss: 5.2896 || 10iter: 4.5661 sec.\n",
      "Iter 990 || Loss: 6.3634 || 10iter: 4.5637 sec.\n",
      "Iter 1000 || Loss: 5.5283 || 10iter: 4.5786 sec.\n",
      "Iter 1010 || Loss: 5.4821 || 10iter: 4.5708 sec.\n",
      "Iter 1020 || Loss: 5.7044 || 10iter: 4.5908 sec.\n",
      "Iter 1030 || Loss: 6.1387 || 10iter: 4.5495 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:7106.3436 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  486.3358 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1040 || Loss: 5.0385 || 10iter: 3.5844 sec.\n",
      "Iter 1050 || Loss: 5.3435 || 10iter: 4.5392 sec.\n",
      "Iter 1060 || Loss: 4.7791 || 10iter: 4.5535 sec.\n",
      "Iter 1070 || Loss: 5.2882 || 10iter: 4.5566 sec.\n",
      "Iter 1080 || Loss: 5.8744 || 10iter: 4.5702 sec.\n",
      "Iter 1090 || Loss: 5.4367 || 10iter: 4.5655 sec.\n",
      "Iter 1100 || Loss: 5.8032 || 10iter: 4.5827 sec.\n",
      "Iter 1110 || Loss: 5.5832 || 10iter: 4.5603 sec.\n",
      "Iter 1120 || Loss: 5.7064 || 10iter: 4.5701 sec.\n",
      "Iter 1130 || Loss: 5.9161 || 10iter: 4.5509 sec.\n",
      "Iter 1140 || Loss: 5.9194 || 10iter: 4.5597 sec.\n",
      "Iter 1150 || Loss: 5.2934 || 10iter: 4.5629 sec.\n",
      "Iter 1160 || Loss: 4.6196 || 10iter: 4.5565 sec.\n",
      "Iter 1170 || Loss: 6.4706 || 10iter: 4.5541 sec.\n",
      "Iter 1180 || Loss: 5.2216 || 10iter: 4.5696 sec.\n",
      "Iter 1190 || Loss: 5.7202 || 10iter: 4.6049 sec.\n",
      "Iter 1200 || Loss: 5.1343 || 10iter: 4.5696 sec.\n",
      "Iter 1210 || Loss: 5.1706 || 10iter: 4.5643 sec.\n",
      "Iter 1220 || Loss: 5.4595 || 10iter: 4.5586 sec.\n",
      "Iter 1230 || Loss: 5.6616 || 10iter: 4.5607 sec.\n",
      "Iter 1240 || Loss: 5.1897 || 10iter: 4.5528 sec.\n",
      "Iter 1250 || Loss: 5.1351 || 10iter: 4.5512 sec.\n",
      "Iter 1260 || Loss: 5.5363 || 10iter: 4.5524 sec.\n",
      "Iter 1270 || Loss: 4.7535 || 10iter: 4.5490 sec.\n",
      "Iter 1280 || Loss: 5.7081 || 10iter: 4.5675 sec.\n",
      "Iter 1290 || Loss: 5.7503 || 10iter: 4.5611 sec.\n",
      "Iter 1300 || Loss: 4.9441 || 10iter: 4.5700 sec.\n",
      "Iter 1310 || Loss: 5.3160 || 10iter: 4.5795 sec.\n",
      "Iter 1320 || Loss: 5.8359 || 10iter: 4.5513 sec.\n",
      "Iter 1330 || Loss: 5.0051 || 10iter: 4.5771 sec.\n",
      "Iter 1340 || Loss: 6.1289 || 10iter: 4.5709 sec.\n",
      "Iter 1350 || Loss: 4.6365 || 10iter: 4.5733 sec.\n",
      "Iter 1360 || Loss: 5.7793 || 10iter: 4.5617 sec.\n",
      "Iter 1370 || Loss: 5.6606 || 10iter: 4.5483 sec.\n",
      "Iter 1380 || Loss: 4.9856 || 10iter: 4.5649 sec.\n",
      "Iter 1390 || Loss: 5.6065 || 10iter: 4.5599 sec.\n",
      "Iter 1400 || Loss: 4.8407 || 10iter: 4.5490 sec.\n",
      "Iter 1410 || Loss: 5.3647 || 10iter: 4.5788 sec.\n",
      "Iter 1420 || Loss: 5.3917 || 10iter: 4.5632 sec.\n",
      "Iter 1430 || Loss: 5.7768 || 10iter: 4.5600 sec.\n",
      "Iter 1440 || Loss: 5.9554 || 10iter: 4.5877 sec.\n",
      "Iter 1450 || Loss: 5.5986 || 10iter: 4.5983 sec.\n",
      "Iter 1460 || Loss: 5.2202 || 10iter: 4.5621 sec.\n",
      "Iter 1470 || Loss: 5.4635 || 10iter: 4.5547 sec.\n",
      "Iter 1480 || Loss: 4.9624 || 10iter: 4.5618 sec.\n",
      "Iter 1490 || Loss: 5.4070 || 10iter: 4.5850 sec.\n",
      "Iter 1500 || Loss: 5.0095 || 10iter: 4.6900 sec.\n",
      "Iter 1510 || Loss: 4.8062 || 10iter: 4.5631 sec.\n",
      "Iter 1520 || Loss: 5.5244 || 10iter: 4.5735 sec.\n",
      "Iter 1530 || Loss: 5.1831 || 10iter: 4.5722 sec.\n",
      "Iter 1540 || Loss: 5.4892 || 10iter: 4.5711 sec.\n",
      "Iter 1550 || Loss: 6.1948 || 10iter: 4.5622 sec.\n",
      "Iter 1560 || Loss: 5.0663 || 10iter: 4.8917 sec.\n",
      "Iter 1570 || Loss: 5.1371 || 10iter: 5.0589 sec.\n",
      "Iter 1580 || Loss: 4.6502 || 10iter: 4.5636 sec.\n",
      "Iter 1590 || Loss: 4.9123 || 10iter: 4.5684 sec.\n",
      "Iter 1600 || Loss: 5.8631 || 10iter: 4.5554 sec.\n",
      "Iter 1610 || Loss: 6.1110 || 10iter: 4.5590 sec.\n",
      "Iter 1620 || Loss: 5.6935 || 10iter: 4.5719 sec.\n",
      "Iter 1630 || Loss: 5.1571 || 10iter: 4.5675 sec.\n",
      "Iter 1640 || Loss: 5.9665 || 10iter: 4.5857 sec.\n",
      "Iter 1650 || Loss: 5.0174 || 10iter: 4.5724 sec.\n",
      "Iter 1660 || Loss: 4.7649 || 10iter: 4.5592 sec.\n",
      "Iter 1670 || Loss: 5.6930 || 10iter: 4.8448 sec.\n",
      "Iter 1680 || Loss: 4.9513 || 10iter: 4.8581 sec.\n",
      "Iter 1690 || Loss: 5.2346 || 10iter: 4.5650 sec.\n",
      "Iter 1700 || Loss: 4.6626 || 10iter: 4.5841 sec.\n",
      "Iter 1710 || Loss: 5.5847 || 10iter: 4.5660 sec.\n",
      "Iter 1720 || Loss: 4.6792 || 10iter: 4.5610 sec.\n",
      "Iter 1730 || Loss: 5.3621 || 10iter: 4.5635 sec.\n",
      "Iter 1740 || Loss: 5.1505 || 10iter: 4.5613 sec.\n",
      "Iter 1750 || Loss: 5.0930 || 10iter: 4.5644 sec.\n",
      "Iter 1760 || Loss: 5.5646 || 10iter: 4.5592 sec.\n",
      "Iter 1770 || Loss: 5.3715 || 10iter: 4.5570 sec.\n",
      "Iter 1780 || Loss: 5.3211 || 10iter: 4.5939 sec.\n",
      "Iter 1790 || Loss: 5.7536 || 10iter: 4.5698 sec.\n",
      "Iter 1800 || Loss: 5.4092 || 10iter: 4.5710 sec.\n",
      "Iter 1810 || Loss: 5.9137 || 10iter: 4.8207 sec.\n",
      "Iter 1820 || Loss: 5.2267 || 10iter: 4.8211 sec.\n",
      "Iter 1830 || Loss: 5.1223 || 10iter: 4.5826 sec.\n",
      "Iter 1840 || Loss: 4.8734 || 10iter: 4.5818 sec.\n",
      "Iter 1850 || Loss: 4.8181 || 10iter: 4.5716 sec.\n",
      "Iter 1860 || Loss: 5.2849 || 10iter: 4.5540 sec.\n",
      "Iter 1870 || Loss: 5.4706 || 10iter: 4.5830 sec.\n",
      "Iter 1880 || Loss: 4.6598 || 10iter: 4.5744 sec.\n",
      "Iter 1890 || Loss: 5.1543 || 10iter: 4.5549 sec.\n",
      "Iter 1900 || Loss: 4.4066 || 10iter: 4.5686 sec.\n",
      "Iter 1910 || Loss: 4.6492 || 10iter: 4.5535 sec.\n",
      "Iter 1920 || Loss: 5.6271 || 10iter: 4.5556 sec.\n",
      "Iter 1930 || Loss: 5.4373 || 10iter: 4.8422 sec.\n",
      "Iter 1940 || Loss: 5.2739 || 10iter: 4.7818 sec.\n",
      "Iter 1950 || Loss: 5.0426 || 10iter: 4.5684 sec.\n",
      "Iter 1960 || Loss: 4.2053 || 10iter: 4.5607 sec.\n",
      "Iter 1970 || Loss: 5.0899 || 10iter: 4.5652 sec.\n",
      "Iter 1980 || Loss: 4.9934 || 10iter: 4.5578 sec.\n",
      "Iter 1990 || Loss: 5.5602 || 10iter: 4.5561 sec.\n",
      "Iter 2000 || Loss: 5.0423 || 10iter: 4.5544 sec.\n",
      "Iter 2010 || Loss: 5.5132 || 10iter: 4.5899 sec.\n",
      "Iter 2020 || Loss: 5.8779 || 10iter: 4.5753 sec.\n",
      "Iter 2030 || Loss: 5.5083 || 10iter: 4.9810 sec.\n",
      "Iter 2040 || Loss: 4.8857 || 10iter: 5.1852 sec.\n",
      "Iter 2050 || Loss: 5.0174 || 10iter: 5.1830 sec.\n",
      "Iter 2060 || Loss: 4.8524 || 10iter: 5.1447 sec.\n",
      "Iter 2070 || Loss: 4.3146 || 10iter: 4.8776 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:5511.2970 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  481.2253 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 2080 || Loss: 5.5089 || 10iter: 6.3515 sec.\n",
      "Iter 2090 || Loss: 4.8466 || 10iter: 5.2169 sec.\n",
      "Iter 2100 || Loss: 5.6269 || 10iter: 5.0930 sec.\n",
      "Iter 2110 || Loss: 4.9574 || 10iter: 5.0051 sec.\n",
      "Iter 2120 || Loss: 5.1781 || 10iter: 5.2014 sec.\n",
      "Iter 2130 || Loss: 5.6346 || 10iter: 5.1641 sec.\n",
      "Iter 2140 || Loss: 4.8912 || 10iter: 5.0207 sec.\n",
      "Iter 2150 || Loss: 4.8875 || 10iter: 5.2521 sec.\n",
      "Iter 2160 || Loss: 5.4583 || 10iter: 5.1015 sec.\n",
      "Iter 2170 || Loss: 5.3827 || 10iter: 5.1044 sec.\n",
      "Iter 2180 || Loss: 4.8258 || 10iter: 5.1126 sec.\n",
      "Iter 2190 || Loss: 5.4307 || 10iter: 5.1381 sec.\n",
      "Iter 2200 || Loss: 4.6317 || 10iter: 5.1750 sec.\n",
      "Iter 2210 || Loss: 4.7511 || 10iter: 4.7829 sec.\n",
      "Iter 2220 || Loss: 4.5864 || 10iter: 4.5782 sec.\n",
      "Iter 2230 || Loss: 5.9185 || 10iter: 4.5619 sec.\n",
      "Iter 2240 || Loss: 5.5019 || 10iter: 4.5904 sec.\n",
      "Iter 2250 || Loss: 5.2542 || 10iter: 4.5543 sec.\n",
      "Iter 2260 || Loss: 5.8319 || 10iter: 4.5685 sec.\n",
      "Iter 2270 || Loss: 4.9092 || 10iter: 4.5821 sec.\n",
      "Iter 2280 || Loss: 4.5344 || 10iter: 4.5712 sec.\n",
      "Iter 2290 || Loss: 5.3036 || 10iter: 4.5598 sec.\n",
      "Iter 2300 || Loss: 4.6179 || 10iter: 4.5454 sec.\n",
      "Iter 2310 || Loss: 5.0066 || 10iter: 4.5530 sec.\n",
      "Iter 2320 || Loss: 5.0214 || 10iter: 4.5677 sec.\n",
      "Iter 2330 || Loss: 4.9441 || 10iter: 4.5635 sec.\n",
      "Iter 2340 || Loss: 5.2908 || 10iter: 4.5627 sec.\n",
      "Iter 2350 || Loss: 4.4346 || 10iter: 4.5799 sec.\n",
      "Iter 2360 || Loss: 4.7247 || 10iter: 4.5692 sec.\n",
      "Iter 2370 || Loss: 4.8051 || 10iter: 4.5745 sec.\n",
      "Iter 2380 || Loss: 4.0833 || 10iter: 4.5766 sec.\n",
      "Iter 2390 || Loss: 4.9064 || 10iter: 4.5660 sec.\n",
      "Iter 2400 || Loss: 4.4211 || 10iter: 4.5520 sec.\n",
      "Iter 2410 || Loss: 3.9828 || 10iter: 4.5535 sec.\n",
      "Iter 2420 || Loss: 3.9995 || 10iter: 4.6248 sec.\n",
      "Iter 2430 || Loss: 4.8093 || 10iter: 4.5660 sec.\n",
      "Iter 2440 || Loss: 4.8500 || 10iter: 4.5736 sec.\n",
      "Iter 2450 || Loss: 5.0337 || 10iter: 4.5640 sec.\n",
      "Iter 2460 || Loss: 5.1724 || 10iter: 4.5775 sec.\n",
      "Iter 2470 || Loss: 4.5418 || 10iter: 4.5687 sec.\n",
      "Iter 2480 || Loss: 5.4707 || 10iter: 4.5746 sec.\n",
      "Iter 2490 || Loss: 5.1814 || 10iter: 4.5811 sec.\n",
      "Iter 2500 || Loss: 5.2351 || 10iter: 4.5682 sec.\n",
      "Iter 2510 || Loss: 4.4806 || 10iter: 4.5553 sec.\n",
      "Iter 2520 || Loss: 4.9527 || 10iter: 4.6024 sec.\n",
      "Iter 2530 || Loss: 5.2072 || 10iter: 4.5833 sec.\n",
      "Iter 2540 || Loss: 3.8473 || 10iter: 4.6106 sec.\n",
      "Iter 2550 || Loss: 4.7767 || 10iter: 4.5495 sec.\n",
      "Iter 2560 || Loss: 4.7276 || 10iter: 4.5537 sec.\n",
      "Iter 2570 || Loss: 3.7149 || 10iter: 4.6046 sec.\n",
      "Iter 2580 || Loss: 5.2361 || 10iter: 4.5631 sec.\n",
      "Iter 2590 || Loss: 4.4906 || 10iter: 4.5597 sec.\n",
      "Iter 2600 || Loss: 6.0430 || 10iter: 4.5711 sec.\n",
      "Iter 2610 || Loss: 4.7667 || 10iter: 4.5661 sec.\n",
      "Iter 2620 || Loss: 5.1452 || 10iter: 4.5642 sec.\n",
      "Iter 2630 || Loss: 3.9903 || 10iter: 4.5662 sec.\n",
      "Iter 2640 || Loss: 5.1283 || 10iter: 4.5616 sec.\n",
      "Iter 2650 || Loss: 4.3232 || 10iter: 4.5779 sec.\n",
      "Iter 2660 || Loss: 5.5796 || 10iter: 4.6002 sec.\n",
      "Iter 2670 || Loss: 5.2478 || 10iter: 4.5817 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
